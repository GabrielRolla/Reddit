{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22df130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo: dataset/comments.csv\n",
      "Arquivo salvo: dataset/subreddits.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Fun√ß√£o para juntar todos os arquivos CSV de uma pasta em um √∫nico DataFrame\n",
    "def juntar_csvs(pasta, arquivo_saida):\n",
    "    arquivos = [os.path.join(pasta, f) for f in os.listdir(pasta) if f.endswith('.csv')]\n",
    "    dfs = [pd.read_csv(f) for f in arquivos]\n",
    "    df_final = pd.concat(dfs, ignore_index=True)\n",
    "    df_final.to_csv(arquivo_saida, index=False)\n",
    "    print(f\"Arquivo salvo: {arquivo_saida}\")\n",
    "\n",
    "# Juntar arquivos de comments\n",
    "juntar_csvs('dataset/comments', 'dataset/comments.csv')\n",
    "\n",
    "# Juntar arquivos de subreddits\n",
    "juntar_csvs('dataset/subreddits', 'dataset/subreddits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c39485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando os datasets...\n",
      "Carregados com sucesso!\n",
      "Posts: 6687 linhas\n",
      "Coment√°rios: 61472 linhas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "print(\"Carregando os datasets...\")\n",
    "\n",
    "# Defina os nomes dos seus arquivos aqui\n",
    "posts_file = 'dataset/subreddits.csv'\n",
    "comments_file = 'dataset/comments.csv'\n",
    "\n",
    "# Carrega os arquivos CSV para DataFrames do pandas\n",
    "try:\n",
    "    df_posts = pd.read_csv(posts_file)\n",
    "    df_comments = pd.read_csv(comments_file)\n",
    "    print(f\"Carregados com sucesso!\")\n",
    "    print(f\"Posts: {len(df_posts)} linhas\")\n",
    "    print(f\"Coment√°rios: {len(df_comments)} linhas\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivos n√£o encontrados. Verifique se os nomes '{posts_file}' e '{comments_file}' est√£o corretos e na pasta 'data'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34371259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removidos 0 posts duplicados.\n",
      "\n",
      "Limpando emojis dos t√≠tulos e corpos dos posts e coment√°rios...\n",
      "Limpeza de emojis conclu√≠da.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>keyword</th>\n",
       "      <th>method</th>\n",
       "      <th>time_filter</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11a27vu</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umapessoaonline</td>\n",
       "      <td>7948</td>\n",
       "      <td>1.00</td>\n",
       "      <td>79</td>\n",
       "      <td>1.677171e+09</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>GPT</td>\n",
       "      <td>praw_search_all</td>\n",
       "      <td>all</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14uyust</td>\n",
       "      <td>Eu_nvr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Priffos</td>\n",
       "      <td>2693</td>\n",
       "      <td>0.99</td>\n",
       "      <td>98</td>\n",
       "      <td>1.688910e+09</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>Algoritmos de IA</td>\n",
       "      <td>praw_search_all</td>\n",
       "      <td>all</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1hmq4rw</td>\n",
       "      <td>Euü§ñnvr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Murilo_Br23</td>\n",
       "      <td>2278</td>\n",
       "      <td>0.99</td>\n",
       "      <td>47</td>\n",
       "      <td>1.735226e+09</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>GPT</td>\n",
       "      <td>praw_search_all</td>\n",
       "      <td>all</td>\n",
       "      <td>2024</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1glbrnm</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RevolutionaryWhale</td>\n",
       "      <td>2216</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15</td>\n",
       "      <td>1.730934e+09</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>Transformers</td>\n",
       "      <td>praw_search_all</td>\n",
       "      <td>all</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1087c0v</td>\n",
       "      <td>Eu_nvr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LordTaw</td>\n",
       "      <td>2065</td>\n",
       "      <td>0.98</td>\n",
       "      <td>48</td>\n",
       "      <td>1.673349e+09</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>Algoritmos de IA</td>\n",
       "      <td>praw_search_all</td>\n",
       "      <td>all</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   title body              author  score  upvote_ratio  \\\n",
       "0  11a27vu  eu_nvr  NaN     umapessoaonline   7948          1.00   \n",
       "1  14uyust  Eu_nvr  NaN             Priffos   2693          0.99   \n",
       "2  1hmq4rw  Euü§ñnvr  NaN         Murilo_Br23   2278          0.99   \n",
       "3  1glbrnm  eu_nvr  NaN  RevolutionaryWhale   2216          1.00   \n",
       "4  1087c0v  Eu_nvr  NaN             LordTaw   2065          0.98   \n",
       "\n",
       "   num_comments   created_utc subreddit           keyword           method  \\\n",
       "0            79  1.677171e+09    eu_nvr               GPT  praw_search_all   \n",
       "1            98  1.688910e+09    eu_nvr  Algoritmos de IA  praw_search_all   \n",
       "2            47  1.735226e+09    eu_nvr               GPT  praw_search_all   \n",
       "3            15  1.730934e+09    eu_nvr      Transformers  praw_search_all   \n",
       "4            48  1.673349e+09    eu_nvr  Algoritmos de IA  praw_search_all   \n",
       "\n",
       "  time_filter  year  month  \n",
       "0         all  2023      2  \n",
       "1         all  2023      7  \n",
       "2         all  2024     12  \n",
       "3         all  2024     11  \n",
       "4         all  2023      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Removendo posts duplicados\n",
    "posts_originais = len(df_posts)\n",
    "df_posts.drop_duplicates(subset=['id'], inplace=True)\n",
    "print(f\"Removidos {posts_originais - len(df_posts)} posts duplicados.\")\n",
    "\n",
    "# 2. Fun√ß√£o para remover emojis\n",
    "def remove_emojis(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Regex que encontra a maioria dos emojis e outros s√≠mbolos\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "print(\"\\nLimpando emojis dos t√≠tulos e corpos dos posts e coment√°rios...\")\n",
    "df_posts['title'] = df_posts['title'].apply(remove_emojis)\n",
    "df_posts['body'] = df_posts['body'].apply(remove_emojis)\n",
    "df_comments['body'] = df_comments['body'].apply(remove_emojis)\n",
    "\n",
    "print(\"Limpeza de emojis conclu√≠da.\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d849e6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a filtragem de posts com corpo vazio e sem coment√°rios...\n",
      "\n",
      "Filtro conclu√≠do!\n",
      "N√∫mero de posts antes da filtragem: 6687\n",
      "N√∫mero de posts ap√≥s a filtragem: 6657\n",
      "Total de 30 posts removidos (corpo vazio e sem coment√°rios).\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando a filtragem de posts com corpo vazio e sem coment√°rios...\")\n",
    "\n",
    "# Primeiro, contamos quantos coment√°rios cada post tem\n",
    "comment_counts = df_comments['post_id'].value_counts().reset_index()\n",
    "comment_counts.columns = ['post_id', 'comment_count']\n",
    "\n",
    "# Agora, juntamos essa contagem de volta ao DataFrame de posts\n",
    "# Usamos 'left' para manter todos os posts, mesmo os que n√£o t√™m coment√°rios (eles ficar√£o com NaN)\n",
    "df_posts = pd.merge(df_posts, comment_counts, left_on='id', right_on='post_id', how='left')\n",
    "\n",
    "# Preenchemos os posts sem coment√°rios com 0\n",
    "df_posts['comment_count'].fillna(0, inplace=True)\n",
    "df_posts['comment_count'] = df_posts['comment_count'].astype(int)\n",
    "\n",
    "# Preenchemos corpos vazios (NaN) com uma string vazia para a verifica√ß√£o\n",
    "df_posts['body'].fillna('', inplace=True)\n",
    "\n",
    "# Armazenamos o n√∫mero de posts antes de filtrar\n",
    "total_antes_filtro = len(df_posts)\n",
    "\n",
    "# Aplicamos o filtro: selecionamos os posts que N√ÉO atendem √† condi√ß√£o de remo√ß√£o\n",
    "# Condi√ß√£o para manter: (corpo N√ÉO √© vazio) OU (contagem de coment√°rios > 0)\n",
    "filtro_manter = (df_posts['body'].str.strip() != '') | (df_posts['comment_count'] > 0)\n",
    "df_posts_filtrado = df_posts[filtro_manter].copy()\n",
    "\n",
    "# Remove as colunas auxiliares que n√£o precisamos mais\n",
    "df_posts_filtrado.drop(columns=['post_id', 'comment_count'], inplace=True)\n",
    "\n",
    "print(f\"\\nFiltro conclu√≠do!\")\n",
    "print(f\"N√∫mero de posts antes da filtragem: {total_antes_filtro}\")\n",
    "print(f\"N√∫mero de posts ap√≥s a filtragem: {len(df_posts_filtrado)}\")\n",
    "print(f\"Total de {total_antes_filtro - len(df_posts_filtrado)} posts removidos (corpo vazio e sem coment√°rios).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9403828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando arquivos limpos em 'dataset/limpos'...\n",
      "Posts limpos salvos em: 'dataset/limpos/posts_limpos.csv'\n",
      "Coment√°rios (com emojis removidos) salvos em: 'dataset/limpos/comments_limpos.csv'\n"
     ]
    }
   ],
   "source": [
    "# Criar o diret√≥rio de sa√≠da se n√£o existir\n",
    "output_dir = 'dataset/limpos'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Nomes dos novos arquivos\n",
    "posts_limpos_file = os.path.join(output_dir, 'posts_limpos.csv')\n",
    "comments_limpos_file = os.path.join(output_dir, 'comments_limpos.csv')\n",
    "\n",
    "print(f\"Salvando arquivos limpos em '{output_dir}'...\")\n",
    "\n",
    "# Salva os dataframes\n",
    "df_posts_filtrado.to_csv(posts_limpos_file, index=False, encoding='utf-8-sig')\n",
    "df_comments.to_csv(comments_limpos_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Posts limpos salvos em: '{posts_limpos_file}'\")\n",
    "print(f\"Coment√°rios (com emojis removidos) salvos em: '{comments_limpos_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dbf645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando os datasets limpos...\n",
      "Arquivos carregados com sucesso!\n",
      "Posts para an√°lise: 6657\n",
      "Coment√°rios para an√°lise: 61472\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configura o pandas para n√£o truncar a exibi√ß√£o das colunas de texto\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Registra o tqdm para poder usar o .progress_apply()\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"Carregando os datasets limpos...\")\n",
    "\n",
    "# Caminhos para os arquivos gerados no passo anterior\n",
    "posts_file = 'dataset/limpos/posts_limpos.csv'\n",
    "comments_file = 'dataset/limpos/comments_limpos.csv'\n",
    "\n",
    "try:\n",
    "    df_posts = pd.read_csv(posts_file)\n",
    "    df_comments = pd.read_csv(comments_file)\n",
    "    print(\"Arquivos carregados com sucesso!\")\n",
    "    print(f\"Posts para an√°lise: {len(df_posts)}\")\n",
    "    print(f\"Coment√°rios para an√°lise: {len(df_comments)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivos n√£o encontrados. Verifique se os arquivos '{posts_file}' e '{comments_file}' existem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8846c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otimizando a estrutura de coment√°rios para busca r√°pida...\n",
      "Estrutura de coment√°rios pronta para verifica√ß√£o.\n"
     ]
    }
   ],
   "source": [
    "print(\"Otimizando a estrutura de coment√°rios para busca r√°pida...\")\n",
    "\n",
    "# Garante que n√£o h√° valores nulos no corpo dos coment√°rios\n",
    "df_comments['body'].fillna('', inplace=True)\n",
    "\n",
    "# Agrupa os coment√°rios por post_id e junta todos os corpos em uma √∫nica string\n",
    "# Usamos .lower() aqui para j√° deixar o texto pronto para a busca case-insensitive\n",
    "comments_grouped = df_comments.groupby('post_id')['body'].apply(lambda x: ' '.join(x).lower())\n",
    "\n",
    "# Converte a S√©rie agrupada para um dicion√°rio para acesso O(1) (muito r√°pido)\n",
    "comments_dict = comments_grouped.to_dict()\n",
    "\n",
    "print(\"Estrutura de coment√°rios pronta para verifica√ß√£o.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba6a2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_relevancia(row):\n",
    "    \"\"\"\n",
    "    Verifica se um post √© relevante.\n",
    "    Retorna True se a keyword exata est√° no post ou em seus coment√°rios.\n",
    "    \"\"\"\n",
    "    # Pega os dados da linha (do post)\n",
    "    post_id = row['id']\n",
    "    keyword = str(row['keyword']).lower() # Keyword do post em min√∫sculo\n",
    "    \n",
    "    # Concatena t√≠tulo e corpo do post, tratando valores nulos e convertendo para min√∫sculo\n",
    "    titulo = str(row['title']).lower() if pd.notna(row['title']) else ''\n",
    "    corpo_post = str(row['body']).lower() if pd.notna(row['body']) else ''\n",
    "    texto_post = titulo + ' ' + corpo_post\n",
    "    \n",
    "    # 1. Verifica se a keyword exata est√° no pr√≥prio post\n",
    "    if keyword in texto_post:\n",
    "        return True\n",
    "    \n",
    "    # 2. Se n√£o estiver, busca a keyword em todos os coment√°rios associados ao post\n",
    "    if post_id in comments_dict:\n",
    "        texto_comentarios = comments_dict[post_id]\n",
    "        if keyword in texto_comentarios:\n",
    "            return True\n",
    "            \n",
    "    # 3. Se n√£o encontrou em nenhum lugar, o post n√£o √© relevante\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12dcbb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando o filtro de relev√¢ncia em todos os posts. Isso pode levar alguns minutos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6657/6657 [00:00<00:00, 20552.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados da Filtragem ---\n",
      "N√∫mero de posts antes da filtragem final: 6657\n",
      "N√∫mero de posts ap√≥s a filtragem final (super filtrado): 1202\n",
      "Total de 5455 posts removidos por falta de relev√¢ncia.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Aplicando o filtro de relev√¢ncia em todos os posts. Isso pode levar alguns minutos...\")\n",
    "\n",
    "# Aplica a fun√ß√£o para criar uma nova coluna booleana ('relevante')\n",
    "df_posts['relevante'] = df_posts.progress_apply(verificar_relevancia, axis=1)\n",
    "\n",
    "# Filtra o DataFrame de posts para manter apenas as linhas marcadas como relevantes\n",
    "df_posts_final = df_posts[df_posts['relevante'] == True].copy()\n",
    "\n",
    "# Remove a coluna auxiliar 'relevante'\n",
    "df_posts_final.drop(columns=['relevante'], inplace=True)\n",
    "\n",
    "print(\"\\n--- Resultados da Filtragem ---\")\n",
    "total_antes = len(df_posts)\n",
    "total_depois = len(df_posts_final)\n",
    "print(f\"N√∫mero de posts antes da filtragem final: {total_antes}\")\n",
    "print(f\"N√∫mero de posts ap√≥s a filtragem final (super filtrado): {total_depois}\")\n",
    "print(f\"Total de {total_antes - total_depois} posts removidos por falta de relev√¢ncia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11a90ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Salvando datasets super filtrados em 'data/super_filtrado'...\n",
      "Posts finais salvos em: 'data/super_filtrado/posts_super_filtrado.csv' (1202 linhas)\n",
      "Coment√°rios finais salvos em: 'data/super_filtrado/comments_super_filtrado.csv' (8202 linhas)\n"
     ]
    }
   ],
   "source": [
    "# Pega a lista de IDs dos posts que foram mantidos\n",
    "ids_relevantes = df_posts_final['id'].unique()\n",
    "\n",
    "# Filtra o DataFrame de coment√°rios original para manter apenas os relevantes\n",
    "df_comments_final = df_comments[df_comments['post_id'].isin(ids_relevantes)].copy()\n",
    "\n",
    "# --- Salvar os arquivos finais ---\n",
    "output_dir = 'data/super_filtrado'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "posts_final_file = os.path.join(output_dir, 'posts_super_filtrado.csv')\n",
    "comments_final_file = os.path.join(output_dir, 'comments_super_filtrado.csv')\n",
    "\n",
    "print(f\"\\nSalvando datasets super filtrados em '{output_dir}'...\")\n",
    "\n",
    "df_posts_final.to_csv(posts_final_file, index=False, encoding='utf-8-sig')\n",
    "df_comments_final.to_csv(comments_final_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Posts finais salvos em: '{posts_final_file}' ({len(df_posts_final)} linhas)\")\n",
    "print(f\"Coment√°rios finais salvos em: '{comments_final_file}' ({len(df_comments_final)} linhas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "724eb64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando datasets super filtrados...\n",
      "Carregados 1202 posts e 8202 coment√°rios.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configura√ß√£o do logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Caminhos para os arquivos gerados na √∫ltima etapa de filtragem\n",
    "POSTS_FILE = 'data/super_filtrado/posts_super_filtrado.csv'\n",
    "COMMENTS_FILE = 'data/super_filtrado/comments_super_filtrado.csv'\n",
    "OUTPUT_FILE = 'data/dataset_unificado_para_analise.csv'\n",
    "\n",
    "print(\"Carregando datasets super filtrados...\")\n",
    "try:\n",
    "    df_posts = pd.read_csv(POSTS_FILE, dtype={'id': str})\n",
    "    df_comments = pd.read_csv(COMMENTS_FILE, dtype={'id': str, 'post_id': str})\n",
    "    print(f\"Carregados {len(df_posts)} posts e {len(df_comments)} coment√°rios.\")\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Erro: Arquivo n√£o encontrado. {e}. Verifique o caminho dos arquivos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94d5df8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 18:31:08,460 - INFO - Iniciando a unifica√ß√£o...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['category'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m df_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Seleciona as colunas que queremos manter para a an√°lise final\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m posts_final \u001b[38;5;241m=\u001b[39m \u001b[43mdf_posts\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdoc_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdoc_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreated_utc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_comments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubreddit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeyword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# --- 2. PREPARA√á√ÉO DOS COMENT√ÅRIOS ---\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Renomeia 'id' para 'doc_id' e 'body' para 'text' para compatibilidade\u001b[39;00m\n\u001b[1;32m     16\u001b[0m df_comments\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['category'] not in index\""
     ]
    }
   ],
   "source": [
    "logging.info(\"Iniciando a unifica√ß√£o...\")\n",
    "\n",
    "# --- 1. PREPARA√á√ÉO DOS POSTS ---\n",
    "# Cria a coluna de texto principal juntando t√≠tulo e corpo\n",
    "df_posts['text'] = df_posts['title'].fillna('') + ' ' + df_posts['body'].fillna('')\n",
    "# Renomeia 'id' para 'doc_id' (ID do Documento) para padronizar\n",
    "df_posts.rename(columns={'id': 'doc_id'}, inplace=True)\n",
    "# Cria uma coluna para identificar o tipo de documento\n",
    "df_posts['doc_type'] = 'post'\n",
    "# Seleciona as colunas que queremos manter para a an√°lise final\n",
    "posts_final = df_posts[['doc_id', 'doc_type', 'text', 'created_utc', 'score', 'num_comments', 'subreddit', 'category', 'keyword']]\n",
    "\n",
    "\n",
    "# --- 2. PREPARA√á√ÉO DOS COMENT√ÅRIOS ---\n",
    "# Renomeia 'id' para 'doc_id' e 'body' para 'text' para compatibilidade\n",
    "df_comments.rename(columns={'id': 'doc_id', 'body': 'text'}, inplace=True)\n",
    "# Adiciona a coluna de tipo de documento\n",
    "df_comments['doc_type'] = 'comment'\n",
    "# Coment√°rios n√£o t√™m \"num_comments\", ent√£o preenchemos com 0 para manter a estrutura\n",
    "df_comments['num_comments'] = 0\n",
    "# Seleciona as colunas finais, mantendo 'post_id' para rastrear a origem\n",
    "comments_final = df_comments[['doc_id', 'doc_type', 'text', 'created_utc', 'score', 'num_comments', 'subreddit', 'post_id', 'keyword']]\n",
    "# Adiciona a coluna 'category' que pode estar faltando nos coment√°rios\n",
    "# (Se precisar, podemos buscar a categoria do post original)\n",
    "if 'category' not in comments_final.columns:\n",
    "    # Mapeia a categoria do post pai para cada coment√°rio\n",
    "    category_map = df_posts.set_index('doc_id')['category']\n",
    "    comments_final['category'] = comments_final['post_id'].map(category_map)\n",
    "\n",
    "\n",
    "# --- 3. CONCATENA√á√ÉO ---\n",
    "df_unificado = pd.concat([posts_final, comments_final], ignore_index=True)\n",
    "logging.info(f\"Unifica√ß√£o completa. O dataset final tem {len(df_unificado)} documentos.\")\n",
    "\n",
    "# --- 4. LIMPEZA LEVE FINAL ---\n",
    "# Aplica a mesma limpeza leve para remover URLs, etc., garantindo consist√™ncia\n",
    "def light_clean(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\[\\s*(removido|deletado)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df_unificado['text_cleaned'] = df_unificado['text'].apply(light_clean)\n",
    "df_unificado = df_unificado[df_unificado['text_cleaned'].str.strip() != ''].copy()\n",
    "\n",
    "# --- 5. SALVAR ---\n",
    "os.makedirs('data', exist_ok=True)\n",
    "df_unificado.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "logging.info(f\"Dataset unificado salvo com sucesso em '{OUTPUT_FILE}'. Total de {len(df_unificado)} linhas prontas para a classifica√ß√£o com LLM.\")\n",
    "\n",
    "df_unificado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60980508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 18:33:17,801 - INFO - Iniciando a unifica√ß√£o...\n",
      "2025-08-06 18:33:17,805 - INFO - Coluna 'category' recriada com sucesso nos posts.\n",
      "2025-08-06 18:33:17,849 - INFO - Unifica√ß√£o completa. O dataset final tem 9404 documentos.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando datasets super filtrados...\n",
      "Carregados 1202 posts e 8202 coment√°rios.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 18:33:18,337 - INFO - Dataset unificado salvo com sucesso em 'data/dataset_unificado_para_analise.csv'. Total de 9337 linhas prontas para a classifica√ß√£o com LLM.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>text</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>category</th>\n",
       "      <th>keyword</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11a27vu</td>\n",
       "      <td>post</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>1.677171e+09</td>\n",
       "      <td>7948</td>\n",
       "      <td>79</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>outro</td>\n",
       "      <td>GPT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eu_nvr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1hmq4rw</td>\n",
       "      <td>post</td>\n",
       "      <td>Euü§ñnvr</td>\n",
       "      <td>1.735226e+09</td>\n",
       "      <td>2278</td>\n",
       "      <td>47</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>outro</td>\n",
       "      <td>GPT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Euü§ñnvr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1glbrnm</td>\n",
       "      <td>post</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>1.730934e+09</td>\n",
       "      <td>2216</td>\n",
       "      <td>15</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>outro</td>\n",
       "      <td>Transformers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eu_nvr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1hl2hd6</td>\n",
       "      <td>post</td>\n",
       "      <td>Eu_NVR</td>\n",
       "      <td>1.735003e+09</td>\n",
       "      <td>1653</td>\n",
       "      <td>23</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>outro</td>\n",
       "      <td>GPT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eu_NVR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18nbq7d</td>\n",
       "      <td>post</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>1.703125e+09</td>\n",
       "      <td>1194</td>\n",
       "      <td>19</td>\n",
       "      <td>eu_nvr</td>\n",
       "      <td>outro</td>\n",
       "      <td>GPT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eu_nvr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id doc_type     text   created_utc  score  num_comments subreddit  \\\n",
       "0  11a27vu     post  eu_nvr   1.677171e+09   7948            79    eu_nvr   \n",
       "1  1hmq4rw     post  Euü§ñnvr   1.735226e+09   2278            47    eu_nvr   \n",
       "2  1glbrnm     post  eu_nvr   1.730934e+09   2216            15    eu_nvr   \n",
       "3  1hl2hd6     post  Eu_NVR   1.735003e+09   1653            23    eu_nvr   \n",
       "4  18nbq7d     post  eu_nvr   1.703125e+09   1194            19    eu_nvr   \n",
       "\n",
       "  category       keyword post_id text_cleaned  \n",
       "0    outro           GPT     NaN       eu_nvr  \n",
       "1    outro           GPT     NaN       Euü§ñnvr  \n",
       "2    outro  Transformers     NaN       eu_nvr  \n",
       "3    outro           GPT     NaN       Eu_NVR  \n",
       "4    outro           GPT     NaN       eu_nvr  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configura√ß√£o do logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Caminhos para os arquivos gerados na √∫ltima etapa de filtragem\n",
    "POSTS_FILE = 'data/super_filtrado/posts_super_filtrado.csv'\n",
    "COMMENTS_FILE = 'data/super_filtrado/comments_super_filtrado.csv'\n",
    "OUTPUT_FILE = 'data/dataset_unificado_para_analise.csv'\n",
    "\n",
    "print(\"Carregando datasets super filtrados...\")\n",
    "try:\n",
    "    df_posts = pd.read_csv(POSTS_FILE, dtype={'id': str})\n",
    "    df_comments = pd.read_csv(COMMENTS_FILE, dtype={'id': str, 'post_id': str})\n",
    "    print(f\"Carregados {len(df_posts)} posts e {len(df_comments)} coment√°rios.\")\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Erro: Arquivo n√£o encontrado. {e}. Verifique o caminho dos arquivos.\")\n",
    "# ==============================================================================\n",
    "#                             IN√çCIO DA CORRE√á√ÉO\n",
    "# ==============================================================================\n",
    "logging.info(\"Iniciando a unifica√ß√£o...\")\n",
    "\n",
    "# --- 1.5 RECRIAR A COLUNA 'category' ---\n",
    "# Definimos aqui o mapeamento original de subreddits para categorias\n",
    "category_mapping = {\n",
    "    'geral': ['brasil', 'brasil2', 'conversas', 'PergunteReddit', 'r/BrasilOnReddit'],\n",
    "    'tecnologia': ['brdev', 'datasciencebr', 'chatgpt_brasil', 'computadores',\n",
    "                   'WindowsBrasil', 'Aplicativo', 'AssistenciaTecnica',\n",
    "                   'Programadores_Alados', 'hardwarebrasil', 'Linuxbrasil', 'programacao']\n",
    "}\n",
    "\n",
    "# Invertemos o dicion√°rio para mapear cada subreddit √† sua categoria\n",
    "subreddit_to_category = {sub: cat for cat, subs in category_mapping.items() for sub in subs}\n",
    "\n",
    "# Criamos a coluna 'category' no DataFrame de posts\n",
    "df_posts['category'] = df_posts['subreddit'].map(subreddit_to_category)\n",
    "# Preenchemos qualquer valor n√£o mapeado como 'outro' para seguran√ßa\n",
    "df_posts['category'].fillna('outro', inplace=True)\n",
    "\n",
    "logging.info(\"Coluna 'category' recriada com sucesso nos posts.\")\n",
    "# ==============================================================================\n",
    "#                              FIM DA CORRE√á√ÉO\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 2. PREPARA√á√ÉO DOS POSTS ---\n",
    "df_posts['text'] = df_posts['title'].fillna('') + ' ' + df_posts['body'].fillna('')\n",
    "df_posts.rename(columns={'id': 'doc_id'}, inplace=True)\n",
    "df_posts['doc_type'] = 'post'\n",
    "posts_final = df_posts[['doc_id', 'doc_type', 'text', 'created_utc', 'score', 'num_comments', 'subreddit', 'category', 'keyword']]\n",
    "\n",
    "\n",
    "# --- 3. PREPARA√á√ÉO DOS COMENT√ÅRIOS ---\n",
    "df_comments.rename(columns={'id': 'doc_id', 'body': 'text'}, inplace=True)\n",
    "df_comments['doc_type'] = 'comment'\n",
    "df_comments['num_comments'] = 0\n",
    "\n",
    "# CORRE√á√ÉO ADICIONAL: Mapeia a categoria do post pai para cada coment√°rio\n",
    "category_map = df_posts.set_index('doc_id')['category']\n",
    "df_comments['category'] = df_comments['post_id'].map(category_map)\n",
    "# Remove coment√°rios cujo post pai n√£o existe mais no df_posts (foram filtrados)\n",
    "df_comments.dropna(subset=['category'], inplace=True)\n",
    "\n",
    "comments_final = df_comments[['doc_id', 'doc_type', 'text', 'created_utc', 'score', 'num_comments', 'subreddit', 'post_id', 'keyword', 'category']]\n",
    "\n",
    "\n",
    "# --- 4. CONCATENA√á√ÉO ---\n",
    "df_unificado = pd.concat([posts_final, comments_final], ignore_index=True)\n",
    "logging.info(f\"Unifica√ß√£o completa. O dataset final tem {len(df_unificado)} documentos.\")\n",
    "\n",
    "# --- 5. LIMPEZA LEVE FINAL ---\n",
    "def light_clean(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\[\\s*(removido|deletado)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df_unificado['text_cleaned'] = df_unificado['text'].apply(light_clean)\n",
    "df_unificado = df_unificado[df_unificado['text_cleaned'].str.strip() != ''].copy()\n",
    "\n",
    "# --- 6. SALVAR ---\n",
    "os.makedirs('data', exist_ok=True)\n",
    "df_unificado.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "logging.info(f\"Dataset unificado salvo com sucesso em '{OUTPUT_FILE}'. Total de {len(df_unificado)} linhas prontas para a classifica√ß√£o com LLM.\")\n",
    "\n",
    "df_unificado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18dcc91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando o dataset unificado para a limpeza final...\n",
      "Dataset carregado com sucesso! Cont√©m 9337 linhas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"Carregando o dataset unificado para a limpeza final...\")\n",
    "\n",
    "# Caminho para o arquivo gerado na etapa anterior\n",
    "UNIFIED_FILE = 'data/dataset_unificado_para_analise.csv'\n",
    "FINAL_OUTPUT_FILE = 'data/dataset_final_pronto_para_llm.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(UNIFIED_FILE, dtype={'doc_id': str, 'post_id': str})\n",
    "    print(f\"Dataset carregado com sucesso! Cont√©m {len(df)} linhas.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo '{UNIFIED_FILE}' n√£o encontrado. Verifique se a etapa anterior foi conclu√≠da com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "767b11f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a remo√ß√£o de posts com a keyword 'agi' e seus respectivos coment√°rios...\n",
      "Encontrados 235 posts para remover (keyword 'agi').\n",
      "Dataset final salvo em: 'data/dataset_final_pronto_para_llm.csv' (8102 linhas)\n",
      "\n",
      "--- Relat√≥rio da Remo√ß√£o ---\n",
      "Linhas antes da remo√ß√£o: 9337\n",
      "Linhas ap√≥s a remo√ß√£o: 8102\n",
      "Total de 1235 linhas removidas.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando a remo√ß√£o de posts com a keyword 'agi' e seus respectivos coment√°rios...\")\n",
    "\n",
    "# 1. Identificar os IDs dos posts que foram capturados pela keyword 'agi'\n",
    "# N√≥s olhamos apenas os documentos do tipo 'post' para pegar o ID original da discuss√£o.\n",
    "post_ids_to_remove = set(df.loc[(df['keyword'] == 'agi') & (df['doc_type'] == 'post'), 'doc_id'])\n",
    "\n",
    "if not post_ids_to_remove:\n",
    "    print(\"Nenhum post encontrado com a keyword 'agi'. Nenhuma remo√ß√£o necess√°ria.\")\n",
    "else:\n",
    "    print(f\"Encontrados {len(post_ids_to_remove)} posts para remover (keyword 'agi').\")\n",
    "\n",
    "    # Armazena o n√∫mero de linhas antes da remo√ß√£o para relat√≥rio\n",
    "    total_antes = len(df)\n",
    "\n",
    "    # 2. Filtrar o DataFrame para remover os posts indesejados E seus coment√°rios\n",
    "    \n",
    "    # Remove os posts cuja keyword √© 'agi'\n",
    "    df_final = df[~df['doc_id'].isin(post_ids_to_remove)]\n",
    "    \n",
    "    # Remove os coment√°rios cujo post_id corresponde a um post removido\n",
    "    # Esta √© a parte crucial, garantindo que os coment√°rios associados tamb√©m sejam removidos.\n",
    "    df_final = df_final[~df_final['post_id'].isin(post_ids_to_remove)]\n",
    "\n",
    "    # 3. Salvar o DataFrame final\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    df_final.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Dataset final salvo em: '{FINAL_OUTPUT_FILE}' ({len(df_final)} linhas)\")\n",
    "\n",
    "    print(\"\\n--- Relat√≥rio da Remo√ß√£o ---\")\n",
    "    print(f\"Linhas antes da remo√ß√£o: {total_antes}\")\n",
    "    print(f\"Linhas ap√≥s a remo√ß√£o: {len(df_final)}\")\n",
    "    print(f\"Total de {total_antes - len(df_final)} linhas removidas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
